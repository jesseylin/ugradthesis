{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "if \"adroit\" in os.uname()[1]:\n",
    "    CUSTOM_MODULE_PATH = \"/home/jylin/thesis/modules\"\n",
    "else:\n",
    "    CUSTOM_MODULE_PATH = \"/System/Volumes/Data/Users/jesselin/Dropbox/src/thesis/modules\"\n",
    "sys.path.append(CUSTOM_MODULE_PATH)\n",
    "\n",
    "# custom libraries\n",
    "from entropy import get_entropy\n",
    "from my_funcs import *\n",
    "\n",
    "# usual libraries\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import functools\n",
    "import time\n",
    "from tqdm import trange, tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# scientific libraries\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Global variables we expect to be set by commandline\n",
    "DATA_DIR = None\n",
    "RES_DIR = None\n",
    "MAX_LOOP_VAR = None\n",
    "TENSORBOARD_LOG_DIRNAME = None\n",
    "EXP_NUM = None\n",
    "TRIAL_NUM = None\n",
    "EXP_DIR = None\n",
    "OUTPUT_DATA_DIR = None\n",
    "_DEFAULT_NUM_SAMPLES = 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# jupyter libraries\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "INTERACTION_Kc = np.log(1+ np.sqrt(2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RES_DIR = \"/Users/jesselin/Dropbox/src/thesis/final/resources\"\n",
    "DATA_DIR = \"/Users/jesselin/Dropbox/src/thesis/final/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(data_filename: str, interaction_K: list, num_samples: int, batch_size: int = 10, num_workers: int = 0):\n",
    "    \"\"\" Takes filename of .npz files for 1D simulations and returns the DataLoader object \"\"\"\n",
    "    dataset = SpinSequenceDataset(data_filename, interaction_K)\n",
    "    div, mod = divmod(len(dataset), num_samples)\n",
    "\n",
    "    # make list of the split proportions\n",
    "    split_list = [num_samples for x in range(div)]\n",
    "    split_list.append(mod)\n",
    "\n",
    "    dataset_after_split = random_split(dataset, split_list)\n",
    "    train_loader = DataLoader(dataset_after_split[0], batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return train_loader\n",
    "\n",
    "def preprocess_data(exp_dir: str):\n",
    "    \"\"\" Stopgap hack to just select the longest iteration file from\n",
    "    one of the experiment directories for 2D old_data. Returns path of file and interaction_K \"\"\"\n",
    "    search_term = os.path.join(exp_dir, \"lattice*.txt\")\n",
    "    raw_file_list = glob.glob(search_term)\n",
    "    data_sample_filepath = raw_file_list[-1]\n",
    "    f = data_sample_filepath.split(\"_\")\n",
    "    temperature_index = f.index(\"T\") + 1\n",
    "    interaction_K = 1/float(f[temperature_index])\n",
    "\n",
    "    return data_sample_filepath, interaction_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_dataloader(dataset: SpinSequenceDataset, num_samples: int, batch_size: int = 10, num_workers: int = 0):\n",
    "    \"\"\" Takes dataset and returns the DataLoader object \"\"\"\n",
    "    div, mod = divmod(len(dataset), num_samples)\n",
    "\n",
    "    # make list of the split proportions\n",
    "    split_list = [num_samples for x in range(div)]\n",
    "    split_list.append(mod)\n",
    "\n",
    "    dataset_after_split = random_split(dataset, split_list)\n",
    "    train_loader = DataLoader(dataset_after_split[0], batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SR"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# for the culture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "data_dir = os.path.join(DATA_DIR, \"1D/SR\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "K = 1.2\n",
    "dataset = SpinSequenceDataset(\"/Users/jesselin/Dropbox/src/thesis/final/resources/1D/SR/trainData_K=1.2.npz\", K)\n",
    "dataloader = make_dataloader(dataset, num_samples=1000)\n",
    "entropy = get_entropy(dimension=1, interaction_K=[1.2, 0., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 100/100 [00:13<00:00,  7.22it/s, loss=0.291, v_num=9]\n"
     ]
    }
   ],
   "source": [
    "for t in np.arange(1)+1:\n",
    "    log_dir = os.path.join(data_dir, f\"macbook_K={K}\")\n",
    "    model = IsingRNN_simple(hidden_size=1, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "    logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size=1\")\n",
    "    early_stop = EarlyStopping(monitor=\"train_loss\", stopping_threshold=entropy*0.995, check_on_train_epoch_end=True, min_delta=0.001, patience=10)\n",
    "    trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "    trainer.fit(model, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# oops, fuckup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_dir = os.path.join(DATA_DIR, \"1D/SR\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "K = 1\n",
    "dataset = SpinSequenceDataset(\"/Users/jesselin/Dropbox/src/thesis/final/resources/1D/SR/trainData_K=1.npz\", K)\n",
    "dataloader = make_dataloader(dataset, num_samples=1000)\n",
    "entropy = get_entropy(dimension=1, interaction_K=[1., 0., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_arm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 100/100 [00:11<00:00,  8.39it/s, loss=0.365, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: 100%|██████████| 100/100 [00:11<00:00,  8.68it/s, loss=0.367, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: 100%|██████████| 100/100 [00:10<00:00,  9.10it/s, loss=0.367, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: 100%|██████████| 100/100 [00:10<00:00,  9.14it/s, loss=0.371, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: 100%|██████████| 100/100 [00:10<00:00,  9.50it/s, loss=0.367, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: 100%|██████████| 100/100 [00:10<00:00,  9.65it/s, loss=0.368, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: 100%|██████████| 100/100 [00:14<00:00,  7.11it/s, loss=0.364, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: 100%|██████████| 100/100 [00:12<00:00,  8.31it/s, loss=0.364, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: 100%|██████████| 100/100 [00:11<00:00,  8.67it/s, loss=0.367, v_num=9]\n"
     ]
    }
   ],
   "source": [
    "for t in np.arange(9)+1:\n",
    "    log_dir = os.path.join(data_dir, f\"macbook_K={K}\")\n",
    "    model = IsingRNN_simple(hidden_size=1, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "    logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size=1\")\n",
    "    early_stop = EarlyStopping(monitor=\"train_loss\", stopping_threshold=entropy, check_on_train_epoch_end=True, min_delta=0.001, patience=10)\n",
    "    trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "    trainer.fit(model, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Low temperature test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data_dir = os.path.join(DATA_DIR, \"1D/SR\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearest_neighbor_bool True\n",
      "k2_bool False\n",
      "k3_bool False\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"1dNNMC_K=2.npz\")[\"data\"]\n",
    "K = 2\n",
    "data = torch.tensor(data)\n",
    "dataset = SpinSequenceDataset(\"1dNNMC_K=2.npz\", K)\n",
    "dataloader = make_dataloader(dataset, num_samples=1000)\n",
    "entropy = get_entropy(dimension=1, interaction_K=[2., 0., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_arm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 100/100 [00:21<00:00,  4.61it/s, loss=0.0936, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 2     \n",
      "2 | fc      | Linear        | 1     \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "3         Trainable params\n",
      "0         Non-trainable params\n",
      "3         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: 100%|██████████| 100/100 [00:17<00:00,  5.76it/s, loss=0.0915, v_num=9]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1\n",
    "for trial in np.arange(2)+1:\n",
    "    log_dir = os.path.join(data_dir, f\"macbook_K={K}\")\n",
    "    model = IsingRNN_simple(hidden_size=hidden_size, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "    logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size={hidden_size}\")\n",
    "    early_stop = EarlyStopping(monitor=\"train_loss\", stopping_threshold=entropy, check_on_train_epoch_end=True, min_delta=0.001, patience=10)\n",
    "    trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "    trainer.fit(model, dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_interaction_K(resource_filepath):\n",
    "    basename, file_ext = os.path.splitext(os.path.basename(resource_filepath))\n",
    "    interaction_K_str = basename[basename.index(\"K=\")+2:]\n",
    "    interaction_K_str = interaction_K_str.strip(\"[]\").split(\" \")\n",
    "    interaction_K = [float(s) for s in interaction_K_str if s != \"\"]\n",
    "    return interaction_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for f in raw_file_list:\n",
    "    interaction_K = get_interaction_K(f)\n",
    "    log_dir = os.path.join(data_dir, f\"macbook_K={str(interaction_K)}\")\n",
    "\n",
    "    dataset = SpinSequenceDataset(f, interaction_K=interaction_K)\n",
    "    dataloader = make_dataloader(dataset, num_samples=100)\n",
    "    for hidden_size in np.arange(5)+1:\n",
    "        model = IsingRNN_simple(hidden_size=hidden_size, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "        logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size={hidden_size}\")\n",
    "        trainer = pl.Trainer(logger=logger, max_epochs=100)\n",
    "        trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeatability of 4-state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "res_dir = os.path.join(RES_DIR, \"1D/LR\")\n",
    "search_term = \"arr_sampledata*\"\n",
    "raw_file_list = glob.glob(os.path.join(res_dir, search_term))\n",
    "\n",
    "data_dir = os.path.join(DATA_DIR, \"1D/LR\")\n",
    "data_filepath = \"/Users/jesselin/Dropbox/src/thesis/final/resources/1D/LR/arr_sampledata_K=[1.  0.1].npz\"\n",
    "K = [1., 0.1, 0.0]\n",
    "dataset = SpinSequenceDataset(data_filepath, K)\n",
    "dataloader = make_dataloader(dataset, num_samples=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hidden_size = 1\n",
    "log_dir = os.path.join(data_dir, f\"macbook_K={K}\")\n",
    "entropy = float(get_entropy(dimension=1, interaction_K=K))\n",
    "for trials in np.arange(10):\n",
    "    model = IsingRNN_simple(hidden_size=hidden_size, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "    logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size={hidden_size}\")\n",
    "    early_stop = EarlyStopping(monitor=\"train_loss\", check_on_train_epoch_end=True, stopping_threshold=entropy, min_delta=0.001, patience=10)\n",
    "    trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "    trainer.fit(model, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lack of 6-state for $k=3$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "res_dir = os.path.join(RES_DIR, \"1D/LR\")\n",
    "search_term = \"arr_sampledata*\"\n",
    "raw_file_list = glob.glob(os.path.join(res_dir, search_term))\n",
    "\n",
    "data_dir = os.path.join(DATA_DIR, \"1D/LR\")\n",
    "data_filepath = \"/Users/jesselin/Dropbox/src/thesis/final/resources/1D/LR/arr_sampledata_K=[1.  0.7 0.2].npz\"\n",
    "K = [1., 0.7, 0.2]\n",
    "dataset = SpinSequenceDataset(data_filepath, K)\n",
    "dataloader = make_dataloader(dataset, num_samples=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for hidden_size in np.arange(2)+1:\n",
    "    log_dir = os.path.join(data_dir, f\"macbook_K={K}\")\n",
    "    entropy = float(get_entropy(dimension=1, interaction_K=K))\n",
    "    for trials in np.arange(10):\n",
    "        model = IsingRNN_simple(hidden_size=hidden_size, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "        logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size={hidden_size}\")\n",
    "        early_stop = EarlyStopping(monitor=\"train_loss\", check_on_train_epoch_end=True, stopping_threshold=entropy, min_delta=0.001, patience=10)\n",
    "        trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "        trainer.fit(model, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfrac_list = 0.9 + np.arange(21)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_dir_2d = os.path.join(RES_DIR, \"2D\")\n",
    "res_dir_compiled_tensor = os.path.join(res_dir_2d, \"compiled tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search_term = \"compiled*\"\n",
    "raw_file_list = glob.glob(os.path.join(res_dir_compiled_tensor, search_term))\n",
    "# assemble dictionary\n",
    "def get_exp_num(filepath):\n",
    "    basename = os.path.basename(f)\n",
    "    basename, file_ext = os.path.splitext(basename)\n",
    "    exp_num = basename.split(\"_\")[4].split(\"=\")[-1]\n",
    "    return int(exp_num)\n",
    "\n",
    "def get_temperature(filepath) -> str:\n",
    "    basename = os.path.basename(f)\n",
    "    basename, file_ext = os.path.splitext(basename)\n",
    "    temperature_str = basename.split(\"_\")[5].split(\"=\")[-1]\n",
    "    return temperature_str\n",
    "\n",
    "file_dict = {}\n",
    "for f in raw_file_list:\n",
    "    temperature = get_temperature(f)\n",
    "    kfrac_str = str(round(1/float(temperature)/INTERACTION_Kc,2))\n",
    "    file_dict[kfrac_str] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfrac = float(1)\n",
    "dataset = SpinSequenceDataset(file_dict[\"1.0\"], interaction_K=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = make_dataloader(dataset, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "test = next(iter(dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "res_dir = os.path.join(RES_DIR, \"2D/compiled extra tensors\")\n",
    "search_term = \"compiled_extra*\"\n",
    "raw_file_list = glob.glob(os.path.join(res_dir, search_term))\n",
    "data_dir = os.path.join(DATA_DIR, \"2D\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "search_term = \"compiled*\"\n",
    "raw_file_list = glob.glob(os.path.join(res_dir, search_term))\n",
    "# assemble dictionary\n",
    "def get_exp_num(filepath):\n",
    "    basename = os.path.basename(f)\n",
    "    basename, file_ext = os.path.splitext(basename)\n",
    "    exp_num = basename.split(\"_\")[4].split(\"=\")[-1]\n",
    "    return int(exp_num)\n",
    "\n",
    "def get_temperature(f):\n",
    "    f, _ = os.path.splitext(f)\n",
    "    temperature = f[f.index(\"T=\")+2:]\n",
    "    return temperature\n",
    "\n",
    "file_dict = {}\n",
    "for f in raw_file_list:\n",
    "    temperature = get_temperature(f)\n",
    "    kfrac_str = str(round(1/float(temperature)/INTERACTION_Kc,2))\n",
    "    file_dict[kfrac_str] = f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "kfrac = float(1)\n",
    "dataset = SpinSequenceDataset(file_dict[\"1.0\"], interaction_K=[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataloader = make_dataloader(dataset, num_samples=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 110   \n",
      "2 | fc      | Linear        | 10    \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "120       Trainable params\n",
      "0         Non-trainable params\n",
      "120       Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_arm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/thesis_arm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 10/10 [00:06<00:00,  1.52it/s, loss=0.381, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "\n",
    "log_dir = os.path.join(data_dir, f\"macbook_kfrac={str(kfrac)}\")\n",
    "model = IsingRNN_simple(hidden_size=hidden_size, num_layers=1, nonlinearity=\"tanh\", bias_on=False)\n",
    "logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size={hidden_size}\")\n",
    "early_stop = EarlyStopping(monitor=\"train_loss\", check_on_train_epoch_end=True, min_delta=0.001, patience=10)\n",
    "trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | loss_fn | NLLLoss       | 0     \n",
      "1 | rnn     | RNN           | 1.9 K \n",
      "2 | fc      | Linear        | 10    \n",
      "3 | logprob | LogisticLayer | 0     \n",
      "------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 0/10 [00:39<?, ?it/s, loss=0.379, v_num=1]_num=2] \n",
      "Epoch 23: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it, loss=0.38, v_num=2] \n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "\n",
    "log_dir = os.path.join(data_dir, f\"macbook_kfrac={str(kfrac)}\")\n",
    "model = IsingRNN_simple(hidden_size=hidden_size, num_layers=10, nonlinearity=\"tanh\", bias_on=False)\n",
    "logger = TensorBoardLogger(save_dir=log_dir, name=f\"hidden_size={hidden_size}\")\n",
    "early_stop = EarlyStopping(monitor=\"train_loss\", check_on_train_epoch_end=True, min_delta=0.001, patience=10)\n",
    "trainer = pl.Trainer(logger=logger, max_epochs=100, callbacks=[early_stop])\n",
    "trainer.fit(model, dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}